{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcZcGaZruTB-",
    "tags": []
   },
   "source": [
    "### **Import the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pvj9AGJtuTB_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score,precision_score,recall_score,f1_score\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeR8oNrJuTCB"
   },
   "source": [
    "## Travel Datasets & Treatment of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x-_l0eJsuTCC"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Traveldata_train.csv\")\n",
    "df1['dataset_type'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94379 entries, 0 to 94378\n",
      "Data columns (total 10 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   ID                       94379 non-null  int64  \n",
      " 1   Gender                   94302 non-null  object \n",
      " 2   Customer_Type            85428 non-null  object \n",
      " 3   Age                      94346 non-null  float64\n",
      " 4   Type_Travel              85153 non-null  object \n",
      " 5   Travel_Class             94379 non-null  object \n",
      " 6   Travel_Distance          94379 non-null  int64  \n",
      " 7   Departure_Delay_in_Mins  94322 non-null  float64\n",
      " 8   Arrival_Delay_in_Mins    94022 non-null  float64\n",
      " 9   dataset_type             94379 non-null  object \n",
      "dtypes: float64(3), int64(2), object(5)\n",
      "memory usage: 7.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Traveldata_test.csv')\n",
    "df2['dataset_type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35602 entries, 0 to 35601\n",
      "Data columns (total 10 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   ID                       35602 non-null  int64  \n",
      " 1   Gender                   35572 non-null  object \n",
      " 2   Customer_Type            32219 non-null  object \n",
      " 3   Age                      35591 non-null  float64\n",
      " 4   Type_Travel              32154 non-null  object \n",
      " 5   Travel_Class             35602 non-null  object \n",
      " 6   Travel_Distance          35602 non-null  int64  \n",
      " 7   Departure_Delay_in_Mins  35573 non-null  float64\n",
      " 8   Arrival_Delay_in_Mins    35479 non-null  float64\n",
      " 9   dataset_type             35602 non-null  object \n",
      "dtypes: float64(3), int64(2), object(5)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union the two tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 129981 entries, 0 to 35601\n",
      "Data columns (total 10 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   ID                       129981 non-null  int64  \n",
      " 1   Gender                   129874 non-null  object \n",
      " 2   Customer_Type            117647 non-null  object \n",
      " 3   Age                      129937 non-null  float64\n",
      " 4   Type_Travel              117307 non-null  object \n",
      " 5   Travel_Class             129981 non-null  object \n",
      " 6   Travel_Distance          129981 non-null  int64  \n",
      " 7   Departure_Delay_in_Mins  129895 non-null  float64\n",
      " 8   Arrival_Delay_in_Mins    129501 non-null  float64\n",
      " 9   dataset_type             129981 non-null  object \n",
      "dtypes: float64(3), int64(2), object(5)\n",
      "memory usage: 10.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_travel.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "u6CaY5DauTCD"
   },
   "outputs": [],
   "source": [
    "travel = df_travel.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8BbtCu5QuTCE"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Customer_Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Type_Travel</th>\n",
       "      <th>Travel_Class</th>\n",
       "      <th>Travel_Distance</th>\n",
       "      <th>Departure_Delay_in_Mins</th>\n",
       "      <th>Arrival_Delay_in_Mins</th>\n",
       "      <th>dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98800001</td>\n",
       "      <td>Female</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Business</td>\n",
       "      <td>272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98800002</td>\n",
       "      <td>Male</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>48.0</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>Eco</td>\n",
       "      <td>2200</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98800003</td>\n",
       "      <td>Female</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>43.0</td>\n",
       "      <td>Business Travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>1061</td>\n",
       "      <td>77.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98800004</td>\n",
       "      <td>Female</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Business Travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>780</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98800005</td>\n",
       "      <td>Female</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Business Travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>1981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Gender   Customer_Type   Age      Type_Travel Travel_Class  \\\n",
       "0  98800001  Female  Loyal Customer  52.0              NaN     Business   \n",
       "1  98800002    Male  Loyal Customer  48.0  Personal Travel          Eco   \n",
       "2  98800003  Female  Loyal Customer  43.0  Business Travel     Business   \n",
       "3  98800004  Female  Loyal Customer  44.0  Business Travel     Business   \n",
       "4  98800005  Female  Loyal Customer  50.0  Business Travel     Business   \n",
       "\n",
       "   Travel_Distance  Departure_Delay_in_Mins  Arrival_Delay_in_Mins  \\\n",
       "0              272                      0.0                    5.0   \n",
       "1             2200                      9.0                    0.0   \n",
       "2             1061                     77.0                  119.0   \n",
       "3              780                     13.0                   18.0   \n",
       "4             1981                      0.0                    0.0   \n",
       "\n",
       "  dataset_type  \n",
       "0        train  \n",
       "1        train  \n",
       "2        train  \n",
       "3        train  \n",
       "4        train  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "travel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lt5d7NJRuTCE"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Customer_Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Type_Travel</th>\n",
       "      <th>Travel_Class</th>\n",
       "      <th>Travel_Distance</th>\n",
       "      <th>Departure_Delay_in_Mins</th>\n",
       "      <th>Arrival_Delay_in_Mins</th>\n",
       "      <th>dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35597</th>\n",
       "      <td>99935598</td>\n",
       "      <td>Male</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>Eco</td>\n",
       "      <td>1334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35598</th>\n",
       "      <td>99935599</td>\n",
       "      <td>Female</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Business Travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>1772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35599</th>\n",
       "      <td>99935600</td>\n",
       "      <td>Male</td>\n",
       "      <td>Disloyal Customer</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Business Travel</td>\n",
       "      <td>Eco</td>\n",
       "      <td>1180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35600</th>\n",
       "      <td>99935601</td>\n",
       "      <td>Female</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>67.0</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>Eco</td>\n",
       "      <td>420</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35601</th>\n",
       "      <td>99935602</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>Eco</td>\n",
       "      <td>1680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  Gender      Customer_Type   Age      Type_Travel  \\\n",
       "35597  99935598    Male     Loyal Customer   8.0  Personal Travel   \n",
       "35598  99935599  Female     Loyal Customer  53.0  Business Travel   \n",
       "35599  99935600    Male  Disloyal Customer  22.0  Business Travel   \n",
       "35600  99935601  Female     Loyal Customer  67.0  Personal Travel   \n",
       "35601  99935602    Male                NaN  20.0  Personal Travel   \n",
       "\n",
       "      Travel_Class  Travel_Distance  Departure_Delay_in_Mins  \\\n",
       "35597          Eco             1334                      0.0   \n",
       "35598     Business             1772                      0.0   \n",
       "35599          Eco             1180                      0.0   \n",
       "35600          Eco              420                     23.0   \n",
       "35601          Eco             1680                      0.0   \n",
       "\n",
       "       Arrival_Delay_in_Mins dataset_type  \n",
       "35597                    0.0         test  \n",
       "35598                    0.0         test  \n",
       "35599                    0.0         test  \n",
       "35600                   16.0         test  \n",
       "35601                    0.0         test  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "travel.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RQwiyX5uTCI"
   },
   "source": [
    "### **Checking for missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BkKU4OT5uTCJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type_Travel                12674\n",
       "Customer_Type              12334\n",
       "Arrival_Delay_in_Mins        480\n",
       "Gender                       107\n",
       "Departure_Delay_in_Mins       86\n",
       "Age                           44\n",
       "ID                             0\n",
       "Travel_Class                   0\n",
       "Travel_Distance                0\n",
       "dataset_type                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_travel = travel.isnull().sum()\n",
    "missing_values_travel.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dteGNwLEuTCJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type_Travel                0.097507\n",
       "Customer_Type              0.094891\n",
       "Arrival_Delay_in_Mins      0.003693\n",
       "Gender                     0.000823\n",
       "Departure_Delay_in_Mins    0.000662\n",
       "Age                        0.000339\n",
       "ID                         0.000000\n",
       "Travel_Class               0.000000\n",
       "Travel_Distance            0.000000\n",
       "dataset_type               0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "share_missing_values_travel = missing_values_travel/travel.isnull().count()\n",
    "share_missing_values_travel.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating missing values of travel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_travel(df):\n",
    "    # Separating categories and numerics \n",
    "    cat_list = df.select_dtypes(['object']).columns.tolist()\n",
    "    num_list = df.select_dtypes(['number']).columns.tolist()\n",
    "\n",
    "    # Replacing NaN values in the data\n",
    "    #10% of values for cust type and travel type are unknown - this could be significant\n",
    "    #df.loc[df['Customer_Type'].isnull(),'Customer_Type'] =  df['Customer_Type'].mode()[0]\n",
    "    df.loc[df['Customer_Type'].isnull(),'Customer_Type'] =  \"Unknown\"\n",
    "    #df.loc[df['Type_Travel'].isnull(),'Type_Travel'] =  df['Type_Travel'].mode()[0]\n",
    "    df.loc[df['Type_Travel'].isnull(),'Type_Travel'] =  \"Unknown\"\n",
    "    # added this \n",
    "    df.loc[df['Gender'].isnull(),'Gender'] =  \"Unknown\"\n",
    "\n",
    "    df.loc[df['Age'].isnull(),'Age'] =  df['Age'].mean()\n",
    "    \n",
    "    df.loc[df['Departure_Delay_in_Mins'].isnull(),'Departure_Delay_in_Mins'] = df['Departure_Delay_in_Mins'].median() \n",
    "    df.loc[df['Arrival_Delay_in_Mins'].isnull(),'Arrival_Delay_in_Mins'] = df['Arrival_Delay_in_Mins'].median() \n",
    "\n",
    "    df.loc[df['Gender'].isnull(),'Gender'] = df['Gender'].mode() \n",
    "    #dropping Gender Na would lose too much info however less than 0.1% of samples are unknown.  we will just use mode\n",
    "    \n",
    "    for col in cat_list:\n",
    "        df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_travel = clean_travel(travel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 129981 entries, 0 to 35601\n",
      "Data columns (total 10 columns):\n",
      " #   Column                   Non-Null Count   Dtype   \n",
      "---  ------                   --------------   -----   \n",
      " 0   ID                       129981 non-null  int64   \n",
      " 1   Gender                   129981 non-null  category\n",
      " 2   Customer_Type            129981 non-null  category\n",
      " 3   Age                      129981 non-null  float64 \n",
      " 4   Type_Travel              129981 non-null  category\n",
      " 5   Travel_Class             129981 non-null  category\n",
      " 6   Travel_Distance          129981 non-null  int64   \n",
      " 7   Departure_Delay_in_Mins  129981 non-null  float64 \n",
      " 8   Arrival_Delay_in_Mins    129981 non-null  float64 \n",
      " 9   dataset_type             129981 non-null  category\n",
      "dtypes: category(5), float64(3), int64(2)\n",
      "memory usage: 6.6 MB\n"
     ]
    }
   ],
   "source": [
    "cleaned_travel.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey Datasets & Treatment of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 =pd.read_csv(\"Surveydata_train.csv\")\n",
    "df3['dataset_type'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94379 entries, 0 to 94378\n",
      "Data columns (total 18 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   ID                       94379 non-null  int64 \n",
      " 1   Overall_Experience       94379 non-null  int64 \n",
      " 2   Seat_Comfort             94318 non-null  object\n",
      " 3   Seat_Class               94379 non-null  object\n",
      " 4   Arrival_Time_Convenient  85449 non-null  object\n",
      " 5   Catering                 85638 non-null  object\n",
      " 6   Platform_Location        94349 non-null  object\n",
      " 7   Onboard_Wifi_Service     94349 non-null  object\n",
      " 8   Onboard_Entertainment    94361 non-null  object\n",
      " 9   Online_Support           94288 non-null  object\n",
      " 10  Ease_of_Online_Booking   94306 non-null  object\n",
      " 11  Onboard_Service          86778 non-null  object\n",
      " 12  Legroom                  94289 non-null  object\n",
      " 13  Baggage_Handling         94237 non-null  object\n",
      " 14  CheckIn_Service          94302 non-null  object\n",
      " 15  Cleanliness              94373 non-null  object\n",
      " 16  Online_Boarding          94373 non-null  object\n",
      " 17  dataset_type             94379 non-null  object\n",
      "dtypes: int64(2), object(16)\n",
      "memory usage: 13.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Overall_Experience\n",
       "0    42786\n",
       "1    51593\n",
       "Name: Overall_Experience, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it's not a 50/50 split balanced but it means that in general in our training data we have more customers who were satisfied (54%)\n",
    "df3.groupby(['Overall_Experience'])['Overall_Experience'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('Surveydata_test.csv')\n",
    "df4['dataset_type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35602 entries, 0 to 35601\n",
      "Data columns (total 17 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   ID                       35602 non-null  int64 \n",
      " 1   Seat_Comfort             35580 non-null  object\n",
      " 2   Seat_Class               35602 non-null  object\n",
      " 3   Arrival_Time_Convenient  32277 non-null  object\n",
      " 4   Catering                 32245 non-null  object\n",
      " 5   Platform_Location        35590 non-null  object\n",
      " 6   Onboard_Wifi_Service     35590 non-null  object\n",
      " 7   Onboard_Entertainment    35594 non-null  object\n",
      " 8   Online_Support           35576 non-null  object\n",
      " 9   Ease_of_Online_Booking   35584 non-null  object\n",
      " 10  Onboard_Service          32730 non-null  object\n",
      " 11  Legroom                  35577 non-null  object\n",
      " 12  Baggage_Handling         35562 non-null  object\n",
      " 13  CheckIn_Service          35580 non-null  object\n",
      " 14  Cleanliness              35600 non-null  object\n",
      " 15  Online_Boarding          35600 non-null  object\n",
      " 16  dataset_type             35602 non-null  object\n",
      "dtypes: int64(1), object(16)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Notice that in the survery data TEST set we don't have the column Overall_Experience. It's normal as our models will need to predict it. \n",
    "# However, we will need to exclude a treatment of missing vsalues for that column when we merge the two datasets.\n",
    "df4.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union the two tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survey = pd.concat([df3, df4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 129981 entries, 0 to 35601\n",
      "Data columns (total 18 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   ID                       129981 non-null  int64  \n",
      " 1   Overall_Experience       94379 non-null   float64\n",
      " 2   Seat_Comfort             129898 non-null  object \n",
      " 3   Seat_Class               129981 non-null  object \n",
      " 4   Arrival_Time_Convenient  117726 non-null  object \n",
      " 5   Catering                 117883 non-null  object \n",
      " 6   Platform_Location        129939 non-null  object \n",
      " 7   Onboard_Wifi_Service     129939 non-null  object \n",
      " 8   Onboard_Entertainment    129955 non-null  object \n",
      " 9   Online_Support           129864 non-null  object \n",
      " 10  Ease_of_Online_Booking   129890 non-null  object \n",
      " 11  Onboard_Service          119508 non-null  object \n",
      " 12  Legroom                  129866 non-null  object \n",
      " 13  Baggage_Handling         129799 non-null  object \n",
      " 14  CheckIn_Service          129882 non-null  object \n",
      " 15  Cleanliness              129973 non-null  object \n",
      " 16  Online_Boarding          129973 non-null  object \n",
      " 17  dataset_type             129981 non-null  object \n",
      "dtypes: float64(1), int64(1), object(16)\n",
      "memory usage: 18.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_survey.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey =df_survey.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Overall_Experience</th>\n",
       "      <th>Seat_Comfort</th>\n",
       "      <th>Seat_Class</th>\n",
       "      <th>Arrival_Time_Convenient</th>\n",
       "      <th>Catering</th>\n",
       "      <th>Platform_Location</th>\n",
       "      <th>Onboard_Wifi_Service</th>\n",
       "      <th>Onboard_Entertainment</th>\n",
       "      <th>Online_Support</th>\n",
       "      <th>Ease_of_Online_Booking</th>\n",
       "      <th>Onboard_Service</th>\n",
       "      <th>Legroom</th>\n",
       "      <th>Baggage_Handling</th>\n",
       "      <th>CheckIn_Service</th>\n",
       "      <th>Cleanliness</th>\n",
       "      <th>Online_Boarding</th>\n",
       "      <th>dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98800001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Green Car</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Very Convenient</td>\n",
       "      <td>Good</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Good</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Poor</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98800002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Ordinary</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Good</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98800003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Green Car</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98800004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Ordinary</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Good</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98800005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Ordinary</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Manageable</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Overall_Experience       Seat_Comfort Seat_Class  \\\n",
       "0  98800001                 0.0  Needs Improvement  Green Car   \n",
       "1  98800002                 0.0               Poor   Ordinary   \n",
       "2  98800003                 1.0  Needs Improvement  Green Car   \n",
       "3  98800004                 0.0         Acceptable   Ordinary   \n",
       "4  98800005                 1.0         Acceptable   Ordinary   \n",
       "\n",
       "  Arrival_Time_Convenient           Catering  Platform_Location  \\\n",
       "0               Excellent          Excellent    Very Convenient   \n",
       "1               Excellent               Poor  Needs Improvement   \n",
       "2       Needs Improvement  Needs Improvement  Needs Improvement   \n",
       "3       Needs Improvement                NaN  Needs Improvement   \n",
       "4              Acceptable         Acceptable         Manageable   \n",
       "\n",
       "  Onboard_Wifi_Service Onboard_Entertainment Online_Support  \\\n",
       "0                 Good     Needs Improvement     Acceptable   \n",
       "1                 Good                  Poor           Good   \n",
       "2    Needs Improvement                  Good      Excellent   \n",
       "3           Acceptable     Needs Improvement     Acceptable   \n",
       "4    Needs Improvement                  Good      Excellent   \n",
       "\n",
       "  Ease_of_Online_Booking    Onboard_Service            Legroom  \\\n",
       "0      Needs Improvement  Needs Improvement         Acceptable   \n",
       "1                   Good          Excellent  Needs Improvement   \n",
       "2              Excellent          Excellent          Excellent   \n",
       "3             Acceptable         Acceptable         Acceptable   \n",
       "4                   Good               Good               Good   \n",
       "\n",
       "    Baggage_Handling    CheckIn_Service        Cleanliness Online_Boarding  \\\n",
       "0  Needs Improvement               Good  Needs Improvement            Poor   \n",
       "1               Poor  Needs Improvement               Good            Good   \n",
       "2          Excellent               Good          Excellent       Excellent   \n",
       "3         Acceptable               Good         Acceptable      Acceptable   \n",
       "4               Good               Good               Good            Good   \n",
       "\n",
       "  dataset_type  \n",
       "0        train  \n",
       "1        train  \n",
       "2        train  \n",
       "3        train  \n",
       "4        train  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Overall_Experience</th>\n",
       "      <th>Seat_Comfort</th>\n",
       "      <th>Seat_Class</th>\n",
       "      <th>Arrival_Time_Convenient</th>\n",
       "      <th>Catering</th>\n",
       "      <th>Platform_Location</th>\n",
       "      <th>Onboard_Wifi_Service</th>\n",
       "      <th>Onboard_Entertainment</th>\n",
       "      <th>Online_Support</th>\n",
       "      <th>Ease_of_Online_Booking</th>\n",
       "      <th>Onboard_Service</th>\n",
       "      <th>Legroom</th>\n",
       "      <th>Baggage_Handling</th>\n",
       "      <th>CheckIn_Service</th>\n",
       "      <th>Cleanliness</th>\n",
       "      <th>Online_Boarding</th>\n",
       "      <th>dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35597</th>\n",
       "      <td>99935598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Green Car</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Manageable</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Good</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35598</th>\n",
       "      <td>99935599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Ordinary</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Good</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35599</th>\n",
       "      <td>99935600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Good</td>\n",
       "      <td>Green Car</td>\n",
       "      <td>Extremely Poor</td>\n",
       "      <td>Good</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Good</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35600</th>\n",
       "      <td>99935601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Ordinary</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Inconvenient</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35601</th>\n",
       "      <td>99935602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Good</td>\n",
       "      <td>Ordinary</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Good</td>\n",
       "      <td>Manageable</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Good</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Needs Improvement</td>\n",
       "      <td>Good</td>\n",
       "      <td>Poor</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  Overall_Experience       Seat_Comfort Seat_Class  \\\n",
       "35597  99935598                 NaN  Needs Improvement  Green Car   \n",
       "35598  99935599                 NaN  Needs Improvement   Ordinary   \n",
       "35599  99935600                 NaN               Good  Green Car   \n",
       "35600  99935601                 NaN          Excellent   Ordinary   \n",
       "35601  99935602                 NaN               Good   Ordinary   \n",
       "\n",
       "      Arrival_Time_Convenient           Catering  Platform_Location  \\\n",
       "35597               Excellent  Needs Improvement         Manageable   \n",
       "35598       Needs Improvement               Good  Needs Improvement   \n",
       "35599          Extremely Poor               Good  Needs Improvement   \n",
       "35600               Excellent          Excellent       Inconvenient   \n",
       "35601              Acceptable               Good         Manageable   \n",
       "\n",
       "      Onboard_Wifi_Service Onboard_Entertainment Online_Support  \\\n",
       "35597           Acceptable     Needs Improvement     Acceptable   \n",
       "35598           Acceptable             Excellent      Excellent   \n",
       "35599    Needs Improvement                  Good           Poor   \n",
       "35600           Acceptable             Excellent           Good   \n",
       "35601                 Poor                  Good           Poor   \n",
       "\n",
       "      Ease_of_Online_Booking Onboard_Service     Legroom Baggage_Handling  \\\n",
       "35597             Acceptable            Good   Excellent             Good   \n",
       "35598                   Good            Good        Good             Good   \n",
       "35599      Needs Improvement            Poor  Acceptable             Poor   \n",
       "35600              Excellent       Excellent   Excellent        Excellent   \n",
       "35601                   Poor      Acceptable        Good             Good   \n",
       "\n",
       "         CheckIn_Service Cleanliness    Online_Boarding dataset_type  \n",
       "35597         Acceptable        Good         Acceptable         test  \n",
       "35598         Acceptable        Good               Good         test  \n",
       "35599               Poor   Excellent  Needs Improvement         test  \n",
       "35600         Acceptable   Excellent               Good         test  \n",
       "35601  Needs Improvement        Good               Poor         test  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Overall_Experience         35602\n",
       "Arrival_Time_Convenient    12255\n",
       "Catering                   12098\n",
       "Onboard_Service            10473\n",
       "Baggage_Handling             182\n",
       "Online_Support               117\n",
       "Legroom                      115\n",
       "CheckIn_Service               99\n",
       "Ease_of_Online_Booking        91\n",
       "Seat_Comfort                  83\n",
       "Platform_Location             42\n",
       "Onboard_Wifi_Service          42\n",
       "Onboard_Entertainment         26\n",
       "Cleanliness                    8\n",
       "Online_Boarding                8\n",
       "ID                             0\n",
       "Seat_Class                     0\n",
       "dataset_type                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_survey = survey.isnull().sum()\n",
    "missing_values_survey.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Overall_Experience         0.273902\n",
       "Arrival_Time_Convenient    0.094283\n",
       "Catering                   0.093075\n",
       "Onboard_Service            0.080573\n",
       "Baggage_Handling           0.001400\n",
       "Online_Support             0.000900\n",
       "Legroom                    0.000885\n",
       "CheckIn_Service            0.000762\n",
       "Ease_of_Online_Booking     0.000700\n",
       "Seat_Comfort               0.000639\n",
       "Platform_Location          0.000323\n",
       "Onboard_Wifi_Service       0.000323\n",
       "Onboard_Entertainment      0.000200\n",
       "Cleanliness                0.000062\n",
       "Online_Boarding            0.000062\n",
       "ID                         0.000000\n",
       "Seat_Class                 0.000000\n",
       "dataset_type               0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "share_missing_values_survey = missing_values_survey/survey.isnull().count()\n",
    "share_missing_values_survey.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating missing values of Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to not treat the values for the test set, we don't have missing values in the train set\n",
    "survey.drop('Overall_Experience',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to do it separately otherwise missing values remain but by doing it before hand it works\n",
    "survey['Platform_Location'].fillna('no answer', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this one cell cleans all the data for the survey train/test\n",
    "def clean_survey(df):\n",
    "    #create a list of ratings columns. Exclude ID and Overall experience\n",
    "    #remove seat class - it is a classifier\n",
    "    #reomve Platform location for separate treatment\n",
    "    cols_rating = df.columns[2:]\n",
    "    cols_rating = cols_rating.drop(['Seat_Class','Platform_Location'])\n",
    "    #fill NA in both train and test\n",
    "    for i in cols_rating:\n",
    "        df[i].fillna('no answer', inplace = True)\n",
    "    #***NOTE - I moved no answer to the middle value instead of the low value.  may want to test both.\n",
    "    #***NOTE - What happens if we increase the weights on thses numbers doubled? squared? etc\n",
    "    #set values to replace\n",
    "    names = [\n",
    "        'Excellent',\n",
    "        'Good',\n",
    "        'Acceptable',\n",
    "        'no answer',\n",
    "        'Needs Improvement',\n",
    "        'Poor',\n",
    "        'Extremely Poor'\n",
    "    ]\n",
    "    #set values to replace for Platform_Location\n",
    "    names_plat = [\n",
    "        'Very Convenient',\n",
    "        'Convenient',\n",
    "        'Manageable',\n",
    "        'no answer',\n",
    "        'Needs Improvement', \n",
    "        'Inconvenient',\n",
    "        'Very Inconvenient',\n",
    "    ]\n",
    "    #numbers that will replace inputs\n",
    "    numbers = [6,5,4,3,2,1,0]\n",
    "    #replace the values in the dataframes\n",
    "    for i in df[cols_rating]:\n",
    "        df[i].replace(to_replace = names, value = numbers, inplace=True)\n",
    "    #Also replace 'Platform_Location' column with numbers\n",
    "    df['Platform_Location'].replace(to_replace = names_plat, value = numbers, inplace=True)\n",
    "    #Set seat class as a category\n",
    "    df['Seat_Class'].astype('category')\n",
    "    df['Platform_Locatio'].astype('category')\n",
    "    for i in cols_rating:\n",
    "        df[i].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Platform_Locatio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Platform_Locatio'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cleaned_survey \u001b[38;5;241m=\u001b[39m \u001b[43mclean_survey\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurvey\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mclean_survey\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#Set seat class as a category\u001b[39;00m\n\u001b[0;32m     41\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeat_Class\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPlatform_Locatio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m cols_rating:\n\u001b[0;32m     44\u001b[0m     df[i]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Platform_Locatio'"
     ]
    }
   ],
   "source": [
    "cleaned_survey = clean_survey(survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_survey.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_train_set = cleaned_survey[cleaned_survey['dataset_type']=='train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_overall_experience = df3[['ID','Overall_Experience']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_train_set = survey_train_set.merge(df3_overall_experience,on='ID',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_train_set.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_test_set = survey[survey['dataset_type']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_survey = pd.concat([survey_train_set, survey_test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_survey.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_survey.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Joining Travel + Survey treated DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleaned_travel.merge(cleaned_survey,on='ID',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.select_dtypes(['object']).columns.tolist()\n",
    "cols.append('Overall_Experience')\n",
    "for i in cols:\n",
    "    df[i] = df[i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHTODkjLuTCT"
   },
   "source": [
    "### **Analyzing Summary Statistics of the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['ID','dataset_type_x','dataset_type_y'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2w7JWBV5uTCT"
   },
   "outputs": [],
   "source": [
    "# Analyzing the summary statistics for numerical variables\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['category']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the count of unique values in each categorical column \n",
    "cols_cat= df.select_dtypes(['category'])\n",
    "\n",
    "for i in cols_cat.columns:\n",
    "    print('Unique values in',i, 'are :')\n",
    "    print(cols_cat[i])\n",
    "    print('Nbr of missing values',cols_cat[i].isnull().sum())    \n",
    "    print(cols_cat[i].value_counts())    \n",
    "    print('*'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_boxplot(feature, figsize=(15,10), bins = None):\n",
    "    \"\"\" Boxplot and histogram combined\n",
    "    feature: 1-d feature array\n",
    "    figsize: size of fig (default (9,8))\n",
    "    bins: number of bins (default None / auto)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2\n",
    "                                           sharex = True, # x-axis will be shared among all subplots                                        \n",
    "                                           gridspec_kw = {\"height_ratios\": (.25, .75)}, \n",
    "                                           figsize = figsize,                                        \n",
    "                                           ) # creating the 2 subplots\n",
    "    print(col)\n",
    "    print('Skew :', round(cols_not_cat[col].skew(), 2))\n",
    "    sns.boxplot(feature, ax=ax_box2, showmeans=True, color='violet') # boxplot will be created and a star will indicate the mean value of the column\n",
    "    sns.distplot(feature, kde=F, ax=ax_hist2, bins=bins,palette=\"winter\") if bins else sns.distplot(feature, kde=False, ax=ax_hist2) # For histogram\n",
    "    ax_hist2.axvline(np.mean(feature), color='green', linestyle='--') # Add mean to the histogram\n",
    "    ax_hist2.axvline(np.median(feature), color='black', linestyle='-') # Add median to the histogram\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_not_cat = df.select_dtypes(exclude=['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_not_cat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_not_cat.columns:\n",
    "    histogram_boxplot(cols_not_cat[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_variables = df.loc[:,df.columns != 'Overall_Experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_not_cat.columns:\n",
    "    sns.boxplot(df[\"Overall_Experience\"],cols_not_cat[col],palette=\"PuBu\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_plot(x):\n",
    "    sns.set(palette='nipy_spectral')\n",
    "    tab1 = pd.crosstab(x,df['Overall_Experience'],margins=True)\n",
    "    tab2 = pd.crosstab(x,df['Overall_Experience'],margins=True,normalize='index')\n",
    "    print(tab1)\n",
    "    print(tab2)    \n",
    "    print('-'*120)\n",
    "    tab = pd.crosstab(x,df['Overall_Experience'],normalize='index')\n",
    "    tab.plot(kind='bar',stacked=True,figsize=(10,5))\n",
    "    plt.legend(loc='lower left', frameon=False)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in independent_variables.columns:\n",
    "    stacked_plot(independent_variables[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating numerical variables\n",
    "numerical_col = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Building correlation matrix for numerical columns\n",
    "corr = df[numerical_col].corr()\n",
    "\n",
    "# ploting the heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr,cmap='coolwarm',vmax=1,vmin=-1, annot = True,\n",
    "        fmt=\".2f\",\n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(df, hue='Overall_Experience')\n",
    "# df[numerical_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier columns \n",
    "outline_col = ['Travel_Distance', 'Departure_Delay_in_Mins', 'Arrival_Delay_in_Mins',     \n",
    "       'Online_Support', 'CheckIn_Service', 'Cleanliness']\n",
    "\n",
    "for col in outline_col:\n",
    "    \n",
    "    print('Skew :', round(df[col].skew(), 2))\n",
    "    \n",
    "    plt.figure(figsize = (15, 4))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "   \n",
    "    plt.title(col + ' Histogram')\n",
    "    \n",
    "    df[col].hist(bins = 10, grid = False)\n",
    "    \n",
    "#     plt.tite\n",
    "    plt.xlabel(col)\n",
    "    \n",
    "    plt.ylabel('count')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    plt.title(col + ' Boxplot')\n",
    "    sns.boxplot(x = df[col])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treatment of Outliers\n",
    "\n",
    "We really need to be carefull here as we have a lot of features that are highly right skewed and some of them could be potential signal for our target variable:\n",
    "\n",
    "- Departure/Arrival delays \n",
    "-> typically, if we go on a trip and the delays are very long we would tend to be disatisfied so we need to think about it, maybe we can cap it or create a new column where we could segment (less than 60 min, 60 min to 120 min, 120 min to 180 min, more than 180 min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Replacing Outliers with Median Values\n",
    "    In this technique, we replace the extreme values with median values. It is advised to not use mean values as they are affected by outliers. The first line of code below prints the 50th percentile value, or the median, which comes out to be 140. The second line prints the 95th percentile value, which comes out to be around 326. The third line of code below replaces all those values in the 'Loan_amount' variable, which are greater than the 95th percentile, with the median value. Finally, the fourth line prints summary statistics after all these techniques have been employed for outlier treatment. (Ref: https://www.pluralsight.com/guides/cleaning-up-data-from-outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_treament(df,col):\n",
    "    middle_num = (df[col].quantile(0.50)) \n",
    "    quartile_95 = (df[col].quantile(0.95)) \n",
    "    print('Median:',middle_num)\n",
    "    treated_col = np.where(df[col] > quartile_95, middle_num, df[col])\n",
    "    return treated_col\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating the cols with the median values. \n",
    "\n",
    "for col_names in outline_col:\n",
    "#     print(col_names)\n",
    "    df[col_names] = outlier_treament(df,col_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns \n",
    "for col in outline_col:\n",
    "    \n",
    "    print('Skew :', round(df[col].skew(), 2))\n",
    "    \n",
    "    plt.figure(figsize = (15, 4))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "   \n",
    "    plt.title(col + ' Histogram')\n",
    "    \n",
    "    df[col].hist(bins = 10, grid = False)\n",
    "    \n",
    "#     plt.tite\n",
    "    plt.xlabel(col)\n",
    "    \n",
    "    plt.ylabel('count')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    plt.title(col + ' Boxplot')\n",
    "    sns.boxplot(x = df[col])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rechecking the number of cats for the category variables\n",
    "for col in df.columns:\n",
    "#     print(trav_train[col].dtypes) #dtypes\n",
    "    if df[col].dtypes == 'category':\n",
    "        print(col,' Categories:',df[col].unique())\n",
    "        print(round((df[col].value_counts()/df.shape[0])*100,3),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggestions \n",
    "## 5.2 Quantile based flooring and capping\n",
    "In this technique, the outlier is capped at a certain value above the 90th percentile value or floored at a factor below the 10th percentile value.\n",
    "    - Reference: https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def treat_outliers(df,col):\n",
    "#     '''\n",
    "#     treats outliers in a variable\n",
    "#     col: str, name of the numerical varaible\n",
    "#     df: data frame\n",
    "#     col: name of the column\n",
    "#     '''\n",
    "    \n",
    "#     Q1=df[col].quantile(0.25) # 25th quantile\n",
    "#     Q3=df[col].quantile(0.75)  # 75th quantile\n",
    "#     IQR=Q3-Q1   # IQR Range\n",
    "#     Lower_Whisker = Q1-IQR*1.5  #define lower whisker\n",
    "#     Upper_Whisker = Q3+IQR*1.5  # define upper Whisker\n",
    "#     df[col] = np.clip(df[col], Lower_Whisker, Upper_Whisker) # all the values samller than Lower_Whisker will be assigned value of Lower_whisker \n",
    "#                                                              # and all the values above upper_whishker will be assigned value of upper_Whisker \n",
    "#     return df\n",
    "\n",
    "# def treat_outliers_all(df, col_list):\n",
    "#     '''\n",
    "#     treat outlier in all numerical varaibles\n",
    "#     col_list: list of numerical varaibles\n",
    "#     df: data frame\n",
    "#     '''\n",
    "#     for c in col_list:\n",
    "#         df = treat_outliers(df,c)\n",
    "        \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the dependent variable from the dataframe and create the X(independent variable) matrix\n",
    "\n",
    "X = df.drop(columns = 'Overall_Experience')\n",
    "# reminder 1 = satisfied, 0 = unsatisfied \n",
    "\n",
    "# Create dummy variables for the categorical variables - Hint: use the get_dummies() function\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "y = df['Overall_Experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Applying fit_transform on the training features data\n",
    "X_scale = scaler.fit_transform(X)\n",
    "\n",
    "# The above scaler returns the data in array format, below we are converting it back to pandas DataFrame\n",
    "X_scale = pd.DataFrame(X_scale, index = X.index, columns = X.columns)\n",
    "\n",
    "#change the X data to the Xscale data\n",
    "X = X_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split out the test and train data sets\n",
    "X = X.loc[df['dataset_type'] = 'train']\n",
    "solution_test = X.loc[df['dataset_type'] = 'test']\n",
    "#drop the test data from y\n",
    "y = y.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating metric function \n",
    "def metrics_score(actual, predicted):\n",
    "    print(classification_report(actual, predicted))\n",
    "    cm = confusion_matrix(actual, predicted)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels=['Not Eligible', 'Eligible'], yticklabels=['Not Eligible', 'Eligible'])\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Logistic Regression model \n",
    "log_reg= LogisticRegression(random_state=1)\n",
    "\n",
    "# Fit the model\n",
    "model = log_reg.fit(X_train, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(log_reg.coef_[0], index = X_train.columns).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict for train set\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "#checking the performance on the test dataset\n",
    "metrics_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict for test set using the optimal threshold\n",
    "y_pred_test = log_reg.predict(X_test)\n",
    "\n",
    "#checking the performance on the test dataset\n",
    "metrics_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba gives the probability of each observation belonging to each class\n",
    "\n",
    "y_scores = log_reg.predict_proba(X_train) \n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores[:,1])\n",
    "\n",
    "# Plotting values of precisions, recalls, and thresholds\n",
    "plt.figure(figsize = (10, 7))\n",
    "\n",
    "plt.plot(thresholds, precisions[:-1], 'b--', label = 'precision')\n",
    "\n",
    "plt.plot(thresholds, recalls[:-1], 'g--', label = 'recall')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "\n",
    "plt.legend(loc = 'upper left')\n",
    "\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the exact threshold where precision and recall are equal\n",
    "for i in np.arange(len(thresholds)):\n",
    "    if precisions[i] == recalls[i]:\n",
    "        optimal_threshold = thresholds[i]\n",
    "        print(\"optimal_threshold is \", optimal_threshold,)\n",
    "\n",
    "metrics_score(y_train, y_scores[:, 1] > optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict for test set using the optimal threshold\n",
    "y_pred_test = log_reg.predict_proba(X_test)\n",
    "\n",
    "#checking the performance on the test dataset\n",
    "metrics_score(y_test, y_pred_test[:, 1] > optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the coefficients of logistic regression\n",
    "# Finding the odds\n",
    "odds = np.exp(log_reg.coef_[0]) \n",
    "\n",
    "# Adding the odds to a dataframe and sorting the values\n",
    "pd.DataFrame(odds, X_train.columns, columns = ['odds']).sort_values(by = 'odds', ascending = False) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "***Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Decision tree model with class weights class_weight={0: 0.2, 1: 0.8}\n",
    "dt = DecisionTreeClassifier(class_weight = {0: 0.2, 1: 0.8}, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting Decision tree model\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the training data\n",
    "\n",
    "y_train_pred_dt = dt.predict(X_train)\n",
    "\n",
    "metrics_score(y_train, y_train_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the testing data\n",
    "y_test_pred_dt = dt.predict(X_test)\n",
    "\n",
    "metrics_score(y_test, y_test_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier\n",
    "dtree_estimator = DecisionTreeClassifier(class_weight = {0: 0.6, 1: 0.4}, random_state = 1,)\n",
    "#this class weight indicates a preference for identifying defaulters\n",
    "# Grid of parameters to choose from\n",
    "parameters = {'max_depth': np.arange(2, 5), \n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'min_samples_leaf': list(range(5,20,5))\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = metrics.make_scorer(recall_score, pos_label = 1)\n",
    "\n",
    "# Run the grid search\n",
    "gridCV = GridSearchCV(dtree_estimator, parameters, scoring = scorer, cv = 10)\n",
    "\n",
    "# Fitting the grid search on the train data\n",
    "gridCV = gridCV.fit(X_train, y_train)\n",
    "\n",
    "# Set the classifier to the best combination of parameters\n",
    "dtree_estimator = gridCV.best_estimator_\n",
    "\n",
    "# Fit the best estimator to the data\n",
    "dtree_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the training data based on the tuned model\n",
    "y_train_pred_dt = dtree_estimator.predict(X_train)\n",
    "\n",
    "metrics_score(y_train, y_train_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the testing data based on the tuned model\n",
    "y_test_pred_dt = dtree_estimator.predict(X_test)\n",
    "\n",
    "metrics_score(y_test, y_test_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = dtree_estimator.feature_importances_\n",
    "\n",
    "columns = X.columns\n",
    "\n",
    "importance_df = pd.DataFrame(importances, index = columns, columns = ['Importance']).sort_values(by = 'Importance', ascending = False)\n",
    "\n",
    "plt.figure(figsize = (13, 13))\n",
    "\n",
    "sns.barplot(importance_df.Importance, importance_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision  tree and analyze it to build the decision rule\n",
    "\n",
    "features = list(X.columns)\n",
    "\n",
    "plt.figure(figsize = (30, 20))\n",
    "\n",
    "tree.plot_tree(dtree_estimator, feature_names = features, filled = True, fontsize = 12, node_ids = True, class_names = class_names)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation = tree.export_text(dtree_estimator,feature_names = features, show_weights=True)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dtreeviz\n",
    "#this is a cool visual I used in my capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtreeviz.trees import dtreeviz # remember to load the package\n",
    "\n",
    "viz = dtreeviz(dtree_estimator, X, y,\n",
    "                target_name=\"BAD\",\n",
    "                feature_names=features,\n",
    "                class_names=class_names,\n",
    "                #orientation='LR'\n",
    "               )\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the training data\n",
    "y_train_pred_dt = dt.predict(X_train)\n",
    "metrics_score(y_train, y_train_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the testing data\n",
    "y_test_pred_dt = dt.predict(X_test)\n",
    "metrics_score(y_test, y_test_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier\n",
    "dtree_estimator = DecisionTreeClassifier(class_weight = {0: 0.6, 1: 0.4}, random_state = 1,)\n",
    "#this class weight indicates a preference for identifying defaulters\n",
    "# Grid of parameters to choose from\n",
    "parameters = {'max_depth': np.arange(2, 5), \n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'min_samples_leaf': list(range(5,20,5))\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = metrics.make_scorer(recall_score, pos_label = 1)\n",
    "\n",
    "# Run the grid search\n",
    "gridCV = GridSearchCV(dtree_estimator, parameters, scoring = scorer, cv = 10)\n",
    "\n",
    "# Fitting the grid search on the train data\n",
    "gridCV = gridCV.fit(X_train, y_train)\n",
    "\n",
    "# Set the classifier to the best combination of parameters\n",
    "dtree_estimator = gridCV.best_estimator_\n",
    "\n",
    "# Fit the best estimator to the data\n",
    "dtree_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the training data based on the tuned model\n",
    "y_train_pred_dt = dtree_estimator.predict(X_train)\n",
    "metrics_score(y_train, y_train_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the testing data based on the tuned model\n",
    "y_test_pred_dt = dtree_estimator.predict(X_test)\n",
    "metrics_score(y_test, y_test_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = dtree_estimator.feature_importances_\n",
    "\n",
    "columns = X.columns\n",
    "\n",
    "importance_df = pd.DataFrame(importances, index = columns, columns = ['Importance']).sort_values(by = 'Importance', ascending = False)\n",
    "\n",
    "plt.figure(figsize = (13, 13))\n",
    "\n",
    "sns.barplot(importance_df.Importance, importance_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision  tree and analyze it to build the decision rule\n",
    "\n",
    "features = list(X.columns)\n",
    "\n",
    "plt.figure(figsize = (30, 20))\n",
    "\n",
    "tree.plot_tree(dtree_estimator, feature_names = features, filled = True, fontsize = 12, node_ids = True, class_names = class_names)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dtreeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtreeviz.trees import dtreeviz # remember to load the package\n",
    "\n",
    "viz = dtreeviz(dtree_estimator, X, y,\n",
    "                target_name=\"BAD\",\n",
    "                feature_names=features,\n",
    "                class_names=class_names,\n",
    "                #orientation='LR'\n",
    "               )\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Random forest CLassifier\n",
    "rf_estimator = RandomForestClassifier(random_state = 1)\n",
    "rf_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking performance on the training data\n",
    "y_pred_train_rf = rf_estimator.predict(X_train)\n",
    "metrics_score(y_train, y_pred_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the test data\n",
    "y_pred_test_rf = rf_estimator.predict(X_test)\n",
    "metrics_score(y_test, y_pred_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Random Forest model with class weights class_weight={0: 0.2, 1: 0.8}\n",
    "rf_estimator_weighted = RandomForestClassifier(class_weight = {0: 0.2, 1: 0.8}, random_state = 1)\n",
    "# Fitting the Random Forest model\n",
    "rf_estimator_weighted.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the train data\n",
    "y_pred_train_rf = rf_estimator_weighted.predict(X_train)\n",
    "metrics_score(y_train, y_pred_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the test data\n",
    "y_pred_test_rf = rf_estimator_weighted.predict(X_test)\n",
    "metrics_score(y_test, y_pred_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {  \n",
    "        \"n_estimators\": [100, 250, 500],\n",
    "        \"min_samples_leaf\": np.arange(1, 4, 1),\n",
    "        \"max_features\": [0.7, 0.9, 'auto'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier\n",
    "rf_estimator_tuned = RandomForestClassifier(class_weight = {0: 0.8, 1: 0.2}, random_state = 1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "params_rf = {  \n",
    "        \"n_estimators\": [100, 250, 500],\n",
    "        \"min_samples_leaf\": np.arange(1, 4, 1),\n",
    "        \"max_features\": [0.7, 0.9, 'auto'],\n",
    "}\n",
    "\n",
    "\n",
    "# Type of scoring used to compare parameter combinations - recall score for class 1\n",
    "scorer = metrics.make_scorer(recall_score, pos_label = 1)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(rf_estimator_tuned, params_rf, scoring = scorer, cv = 5)\n",
    "\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the classifier to the best combination of parameters\n",
    "rf_estimator_tuned = grid_obj.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the train data\n",
    "y_pred_train_rf = rf_estimator_tuned.predict(X_train)\n",
    "\n",
    "metrics_score(y_train, y_pred_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the test data\n",
    "y_pred_test_rf = rf_estimator_tuned.predict(X_test)\n",
    "\n",
    "metrics_score(y_test, y_pred_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance of features in the tree building ( The importance of a feature is computed as the \n",
    "#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n",
    "# Checking performace on test dataset\n",
    "importances = rf_estimator_tuned.feature_importances_\n",
    "\n",
    "columns = X.columns\n",
    "\n",
    "importance_df = pd.DataFrame(importances, index = columns, columns = ['Importance']).sort_values(by = 'Importance', ascending = False)\n",
    "\n",
    "plt.figure(figsize = (13, 13))\n",
    "\n",
    "sns.barplot(importance_df.Importance, importance_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall_score(model,flag=True,X_train=X_train,X_test=X_test):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "\n",
    "    '''\n",
    "    a = [] # defining an empty list to store train and test results\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    train_recall = metrics.recall_score(y_train,pred_train)\n",
    "    test_recall = metrics.recall_score(y_test,pred_test)\n",
    "    a.append(train_recall) # adding train recall to list \n",
    "    a.append(test_recall) # adding test recall to list\n",
    "    \n",
    "    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n",
    "    if flag == True: \n",
    "        print(\"Recall on training set : \",metrics.recall_score(y_train,pred_train))\n",
    "        print(\"Recall on test set : \",metrics.recall_score(y_test,pred_test))\n",
    "    \n",
    "    return a # returning the list with train and test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Function to calculate precision score\n",
    "def get_precision_score(model,flag=True,X_train=X_train,X_test=X_test):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "\n",
    "    '''\n",
    "    b = []  # defining an empty list to store train and test results\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    train_precision = metrics.precision_score(y_train,pred_train)\n",
    "    test_precision = metrics.precision_score(y_test,pred_test)\n",
    "    b.append(train_precision) # adding train precision to list\n",
    "    b.append(test_precision) # adding test precision to list\n",
    "    \n",
    "    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n",
    "    if flag == True: \n",
    "        print(\"Precision on training set : \",metrics.precision_score(y_train,pred_train))\n",
    "        print(\"Precision on test set : \",metrics.precision_score(y_test,pred_test))\n",
    "\n",
    "    return b # returning the list with train and test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Function to calculate accuracy score\n",
    "def get_accuracy_score(model,flag=True,X_train=X_train,X_test=X_test):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "\n",
    "    '''\n",
    "    c = [] # defining an empty list to store train and test results\n",
    "    train_acc = model.score(X_train,y_train)\n",
    "    test_acc = model.score(X_test,y_test)\n",
    "    c.append(train_acc) # adding train accuracy to list\n",
    "    c.append(test_acc) # adding test accuracy to list\n",
    "    \n",
    "    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n",
    "    if flag == True:\n",
    "        print(\"Accuracy on training set : \",model.score(X_train,y_train))\n",
    "        print(\"Accuracy on test set : \",model.score(X_test,y_test))\n",
    "    \n",
    "    return c # returning the list with train and test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the list of all the model names \n",
    "\n",
    "models = [rf_estimator_tuned, dtree_estimator, log_reg]\n",
    "\n",
    "\n",
    "# defining empty lists to add train and test results\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "recall_train = []\n",
    "recall_test = []\n",
    "precision_train = []\n",
    "precision_test = []\n",
    "\n",
    "# looping through all the models to get the accuracy,recall and precision scores\n",
    "for model in models:\n",
    "     # accuracy score\n",
    "    j = get_accuracy_score(model,False)\n",
    "    acc_train.append(j[0])\n",
    "    acc_test.append(j[1])\n",
    "\n",
    "    # recall score\n",
    "    k = get_recall_score(model,False)\n",
    "    recall_train.append(k[0])\n",
    "    recall_test.append(k[1])\n",
    "\n",
    "    # precision score\n",
    "    l = get_precision_score(model,False)\n",
    "    precision_train.append(l[0])\n",
    "    precision_test.append(l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mention the Model names in the list. for example 'Model': ['Decision Tree', 'Tuned Decision Tree'..... write tht names of all model built]\n",
    "\n",
    "\n",
    "comparison_frame = pd.DataFrame({'Model':['Random Forest', 'Decision Tree', 'Logistic Regression'], \n",
    "                                          'Train_Accuracy': acc_train,\n",
    "                                          'Test_Accuracy': acc_test,\n",
    "                                          'Train_Recall': recall_train,\n",
    "                                          'Test_Recall': recall_test,\n",
    "                                          'Train_Precision': precision_train,\n",
    "                                          'Test_Precision': precision_test}) \n",
    "comparison_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MeR8oNrJuTCB",
    "T9ykJzCRuTCD",
    "XJKmSsjtuTCF",
    "HGRNlowYuTCG",
    "8RQwiyX5uTCI",
    "MEwi3CxXuTCL",
    "PgMjKOaRuTCM",
    "rHTODkjLuTCT",
    "KVqJU9nluTCV",
    "3ZcMbNvZuTCW",
    "4whDGVaBuTCW",
    "blkJiWpZuTCX",
    "NQPk1YdYuTCY",
    "35i8LgeWuTCY",
    "XtWPhW6AuTCZ",
    "igI2dP6wuTCa",
    "bj9lXxzWuTCa",
    "RcrXKyzxuTCb",
    "e5Uz1rqnuTCc",
    "1eucYnvKuTCc",
    "cAs34FMbuTCd",
    "ce5BjPB1uTCf",
    "bv2ZSN0fuTCj",
    "fq0ReWniuTCl",
    "pc9wZJcGuTCm",
    "RTN4DIW4uTCp",
    "zEjMlq0quTCp",
    "Jy520Nv-uTCq",
    "_tXbx-9cuTCr"
   ],
   "name": "Reference_Notebook_Milestone_1_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
